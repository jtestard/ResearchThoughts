### Comparison between Hadapt and Polybase

	- Polybase's interface is the PDW product microsoft is selling. Hadoop is viewed as a secondary data source. Hadoop data has to be "registered" with the database through an external table mechanism. As such, the hadoop data must be somewhat structured (it cannot be truly arbitrary given the schema constraints imposed by the external table). Moreover, if we look at a plot of the query time and where it is spent, the vast majority of the time is spent on import data from Hadoop to SQL PDW. In other words, it would have been better to have kept the data in PDW all along.
	  - As such, Polybase is better for accessing data from Hadoop when queries that access Hadoop data are unfrequent, since the Hadoop data is kept out of the database unless it is specifically required by a query.
	- In Hadapt, if the data is structured enough to fit in the DBMS, we only pay a one time load cost. Subsequent queries do not have to worry about data import ( ==> what about updates?).
	  - The decision to have data stored in HDFS or SQL does not depend on the number of time this data is accessed, but whether that data is structured (or semistructured) or completely unstructured.    
 
### CSE 202 project

Join Size estimation for relations R1 to Rn.
The number of values which will remain for attribute A after the join will be the smallest of the V(R,a)'s. In other words, V(R \join S, A) = min(V(R,a),V(S,a))

I need create a join order trees class
I need to have a way to get the size for a join order tree.

joinOrdering(r=r1,...,rn):
	if |r| = 1:
		r1 = r[0]
		size = r1.size
		cost = 0
		formula = r1
	if |r| = 2:
		r1 = r[0]
		r2 = r[1]
		cost = 0
		size = (r1.size*r2.size)/(max(r1.distinct,r2.distinct))
		if (r1 < r2) then formula = [r1,r2] else formula=[r2,r1]
	if |r| > 2:
		for k=3 to |r|:
			for each s \subset r s.t. |s| = k
				for each t in s:
					cost = cost(t-s) + size(t)0000000000 
	return cost, size, formula 